# Story Writing Agent

A modern story writing assistant designed to help authors manage multiple stories, distinct story bibles, and chapter drafts with version control and persistence.

## Project Purpose

The goal of this project is to create a robust, local-first story writing environment that bridges the gap between traditional writing software and AI-assisted creativity. It is architected to eventually support AI generation of story elements and chapters via local LLMs (like LMStudio) or OpenAI-compatible APIs.

## Features

-   **Multi-Story Support**: Create and switch between multiple independent stories.
-   **Story Bible**: Manage characters, locations, timelines, and narrative arcs with customizable fields.
-   **Chapter Management**: Write and organize chapters with support for reordering.
-   **Version Control**: granular history tracking for both chapters and story bible elements, allowing you to view and revert to previous versions.
-   **Soft Delete**: "Recycle bin" functionality prevents accidental data loss.
-   **AI Writing Assistant**: Integrated capability to work with local LLMs (like Ollama) to draft chapters using the Story Bible for context.
-   **Local Persistence**: All data is stored locally in a SQLite database.
-   **Dark Mode**: A modern, dark-themed UI optimized for writing.
-   **Cross-Referencing**: Refer to bible elements within your text (e.g., `[[Character:Name]]`) for quick context.

## AI Integration

The Story Writing Agent supports local LLM integration to assist with drafting chapters.

### Configuration
In the **Global Settings** view, you can configure:
-   **LLM Endpoint URL**: The API endpoint for your local LLM (e.g., `http://localhost:11434/api/generate` for Ollama).
-   **Global System Prompt**: A central instruction set for the AI assistant.

### How it Works
1.  **Context Enrichment**: When you use the "Write with AI" feature, the agent automatically bundles your current Story Bible elements into the prompt, ensuring the AI is aware of your characters, locations, and world-building.
2.  **Chapter Generation**: From the chapter editor, click **ðŸª„ Write with AI**, describe what you want the chapter (or scene) to be about, and the agent will call your local LLM to generate the content.

## Technology Stack

### Backend
-   **Language**: Python 3.x
-   **Framework**: [FastAPI](https://fastapi.tiangolo.com/)
-   **Database**: SQLite (via [SQLModel](https://sqlmodel.tiangolo.com/))
-   **Key Libraries**: `uvicorn` (server), `pydantic` (validation)
-   **Architecture**: RESTful API with distinct layers for Models, CRUD operations, and Route handlers.

### Frontend
-   **Framework**: [React](https://react.dev/) (Vite)
-   **Styling**: Vanilla CSS (Modern CSS3 variables for theming)
-   **HTTP Client**: Native `fetch` / Async logic
-   **Architecture**: Component-based React application (currently centralized in `App.jsx` for rapid prototyping, with specialized components like `SlashEditor`).

## Architecture Overview

The application follows a standard Client-Server architecture:

1.  **Frontend (Client)**: A React SPA (Single Page Application) that communicates with the backend via JSON REST APIs. It handles the UI state, routing (view switching), and interactions.
2.  **Backend (Server)**: A FastAPI application serving at `http://localhost:8000`. It manages business logic, data validation, and database interactions.
3.  **Database**: A local `story_agent.db` SQLite file generated automatically by the backend.

### Data Models
-   **Story**: The root container.
-   **BibleElement**: Entities associated with a story (Characters, Locations, etc.). Contains a JSON-flexible structure for different types.
-   **Chapter**: Narrative units of a story, ordered by index.
-   **VersionHistory**: Immutable snapshots of *Chapters* or *BibleElements* taken on every save, storing the content and timestamp.
-   **GlobalSetting**: System-wide configurations (e.g., default forms/schemas for new bible elements).

## Project Structure

```
story-agent/
â”œâ”€â”€ backend/                # Python FastAPI Backend
â”‚   â”œâ”€â”€ main.py             # API Entry point & Routes
â”‚   â”œâ”€â”€ models.py           # SQLModel Database Schemas
â”‚   â”œâ”€â”€ crud.py             # Database Operation logic
â”‚   â”œâ”€â”€ database.py         # DB Connection setup
â”‚   â””â”€â”€ story_agent.db      # Local SQLite Database (generated)
â”œâ”€â”€ frontend/               # React Frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx         # Main UI Logic & Routing
â”‚   â”‚   â”œâ”€â”€ App.css         # Main Styles
â”‚   â”‚   â””â”€â”€ SlashEditor.jsx # specialized editor component
â”‚   â””â”€â”€ package.json
â””â”€â”€ README.md
```

## Getting Started

### Prerequisites
-   Python 3.8+
-   Node.js 16+ & npm

### Installation relative to root

1.  **Backend Setup**:
    ```bash
    cd backend
    python3 -m venv venv
    source venv/bin/activate  # On Windows: venv\Scripts\activate
    pip install fastapi "uvicorn[standard]" sqlmodel
    ```

2.  **Frontend Setup**:
    ```bash
    cd frontend
    npm install
    ```

### Running the Application

You need to run the backend and frontend in separate terminal windows.

**Terminal 1 (Backend):**
```bash
cd backend
source venv/bin/activate
uvicorn main:app --reload
```
*API will run at http://localhost:8000*

**Terminal 2 (Frontend):**
```bash
cd frontend
npm run dev
```
*UI will run at http://localhost:3000* (or similar port shown in terminal)

## Future Roadmap

-   **AI Integration**: Connect to local LLMs (via LMStudio) to generate character sheets, plot outlines, and draft chapters based on bible context.
-   **Advanced Export**: Export stories to standard formats (PDF, ePub, Docx).
